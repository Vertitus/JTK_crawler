max_concurrent: 8
max_retries: 3
max_depth: 3
queue_size: 10000
auto_save_interval: 300
batch_size: 1000
cache_dir: 'cache'
log:
  path: 'logs/crawler.log'
  max_bytes: 10485760
  backup_count: 5
fetch:
  user_agents_file: "user_agents.txt"
  rate_limit: 1
storage:
  bloom_capacity: 1000000
  bloom_error_rate: 0.001
  cache_ttl_days: 7
  cache_dir: "cache"
parser:
  patterns_file: "keywords.txt"   # Совпадает с именем поля в классе
  url_filters: "url_filters.txt"  # Совпадает с именем поля
  case_sensitive: false 
scheduler:
  debug: false 
  seeds:
    - "fileman.n1e.jp"
    - "2ch.net"
    - "0ch.net"
    - "geocities.jp"
    - "yaplog.jp"
    - "nifty.com"
    - "ocn.ne.jp"
    - "biglobe.ne.jp"
    - "pya.cc"
  poison_pill: "STOP"
  max_concurrent: 8
  max_depth: 3
  queue_size: 10000
cdx:
  request_timeout: 30       # Таймаут запросов к CDX API (сек)
  max_pages: 0   
  page_size: 500000         # Макс. страниц результатов (5000 URL на страницу)
  backoff_factor: 2.0      # Экспоненциальная задержка при повторах
  target_domains_file: "domains.txt"  # Путь к файлу с доменами
  max_retries: 3 